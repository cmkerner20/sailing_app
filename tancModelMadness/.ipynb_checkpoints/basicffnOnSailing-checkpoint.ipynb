{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = data.Field(unk_token = None)\n",
    "TAGS.build_vocab(['0','S', 'C','G','T','B','D', 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/Users/tancredicp/Desktop/sailing_app/data/ML Training Resources/'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "Data = []\n",
    "for filename in all_files:\n",
    "    temporary_dict = {}\n",
    "\n",
    "    dfSpecific = pd.read_csv(filename, index_col=None, header=0)\n",
    "    dfSpecific.fillna(0, inplace= True)\n",
    "    \n",
    "    dfSpecificY = dfSpecific['tag']\n",
    "    temporary_dict['tags'] = TAGS.numericalize((np.array(dfSpecificY)))\n",
    "    \n",
    "\n",
    "    dfSpecificX = dfSpecific.drop(columns=['tag', 'tag_name', 'boat_class', 'time'])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    columnsBeingStandardize = ['lat', 'lon', 'sog', 'heel', 'clew_load','pitch']\n",
    "    dfSpecificX[columnsBeingStandardize] = scaler.fit_transform(dfSpecificX[columnsBeingStandardize])\n",
    "    \n",
    "    \n",
    "    array1 = np.array(dfSpecificX[['lat', 'lon', 'sog', 'cog']])\n",
    "    \n",
    "    temp = torch.from_numpy(array1)\n",
    "    \n",
    "    temporary_dict['track'] = temp\n",
    "    \n",
    "    Data.append(temporary_dict)\n",
    "    del(temporary_dict)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SailingData(Dataset):\n",
    "    def __init__ (self, pl):\n",
    "        self.samples = []\n",
    "        \n",
    "        for d in pl:\n",
    "            for track, tag in zip(d['track'], d['tags'][0]):\n",
    "                self.samples.append((track, tag))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "            return(self.samples[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1#the automatic batch size is 1\n",
    "DatasetSailing= SailingData(Data)\n",
    "train_iterator = torch.utils.data.DataLoader(DatasetSailing, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.5262, -0.5372,  1.5119, -0.3111], dtype=torch.float64), tensor(7))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetSailing[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TAGS.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 120)\n",
    "        self.fc2 = nn.Linear(120, 120)\n",
    "        self.fc3 = nn.Linear(120,9)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net = net.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9225721784776902"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(o_lst, l_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  5768.379207203143 Accuracy:  0.7921535893155259\n",
      "Epoch:  1 Loss:  5446.220315105982 Accuracy:  0.85920979410128\n",
      "Epoch:  2 Loss:  5440.085261671464 Accuracy:  0.8603227601558152\n",
      "Epoch:  3 Loss:  5437.8176908094965 Accuracy:  0.8600445186421815\n",
      "Epoch:  4 Loss:  5435.265276928381 Accuracy:  0.85920979410128\n",
      "Epoch:  5 Loss:  5432.364063019704 Accuracy:  0.8606010016694491\n",
      "Epoch:  6 Loss:  5429.038555696232 Accuracy:  0.8614357262103506\n",
      "Epoch:  7 Loss:  5425.2945460975725 Accuracy:  0.8619922092376182\n",
      "Epoch:  8 Loss:  5421.11883860455 Accuracy:  0.8639398998330551\n",
      "Epoch:  9 Loss:  5416.46362703669 Accuracy:  0.8653311074012242\n",
      "Epoch:  10 Loss:  5412.876269544714 Accuracy:  0.8670005564830273\n",
      "Epoch:  11 Loss:  5410.25327522567 Accuracy:  0.867557039510295\n",
      "Epoch:  12 Loss:  5408.162450289514 Accuracy:  0.8678352810239288\n",
      "Epoch:  13 Loss:  5406.58634805601 Accuracy:  0.8683917640511964\n",
      "Epoch:  14 Loss:  5405.421844093847 Accuracy:  0.8686700055648303\n",
      "Epoch:  15 Loss:  5404.5360818090285 Accuracy:  0.869226488592098\n",
      "Epoch:  16 Loss:  5403.92165042824 Accuracy:  0.8697829716193656\n",
      "Epoch:  17 Loss:  5403.441657871344 Accuracy:  0.8697829716193656\n",
      "Epoch:  18 Loss:  5403.0771656721745 Accuracy:  0.8697829716193656\n",
      "Epoch:  19 Loss:  5402.795499675414 Accuracy:  0.8695047301057318\n",
      "Epoch:  20 Loss:  5402.52611139787 Accuracy:  0.8695047301057318\n",
      "Epoch:  21 Loss:  5402.271114963404 Accuracy:  0.8695047301057318\n",
      "Epoch:  22 Loss:  5402.0289089871585 Accuracy:  0.869226488592098\n",
      "Epoch:  23 Loss:  5401.613861320971 Accuracy:  0.8689482470784641\n",
      "Epoch:  24 Loss:  5401.240402411909 Accuracy:  0.8689482470784641\n",
      "Epoch:  25 Loss:  5400.657627559669 Accuracy:  0.8689482470784641\n",
      "Epoch:  26 Loss:  5400.161870979161 Accuracy:  0.8695047301057318\n",
      "Epoch:  27 Loss:  5399.641979443058 Accuracy:  0.8700612131329994\n",
      "Epoch:  28 Loss:  5399.244107565892 Accuracy:  0.8703394546466333\n",
      "Epoch:  29 Loss:  5398.830434472887 Accuracy:  0.8700612131329994\n",
      "Epoch:  30 Loss:  5398.556958550671 Accuracy:  0.8706176961602671\n",
      "Epoch:  31 Loss:  5398.193595814626 Accuracy:  0.8703394546466333\n",
      "Epoch:  32 Loss:  5397.961789671639 Accuracy:  0.8706176961602671\n",
      "Epoch:  33 Loss:  5397.654278573395 Accuracy:  0.8706176961602671\n",
      "Epoch:  34 Loss:  5397.489736318745 Accuracy:  0.8706176961602671\n",
      "Epoch:  35 Loss:  5397.346445735833 Accuracy:  0.8706176961602671\n",
      "Epoch:  36 Loss:  5397.172991718637 Accuracy:  0.8706176961602671\n",
      "Epoch:  37 Loss:  5397.1093660171455 Accuracy:  0.8706176961602671\n",
      "Epoch:  38 Loss:  5396.969979259177 Accuracy:  0.8706176961602671\n",
      "Epoch:  39 Loss:  5396.871734630014 Accuracy:  0.8708959376739009\n",
      "Epoch:  40 Loss:  5396.843290245688 Accuracy:  0.8708959376739009\n",
      "Epoch:  41 Loss:  5396.651877626746 Accuracy:  0.8706176961602671\n",
      "Epoch:  42 Loss:  5396.576013492732 Accuracy:  0.8706176961602671\n",
      "Epoch:  43 Loss:  5396.577599527784 Accuracy:  0.8706176961602671\n",
      "Epoch:  44 Loss:  5396.414851171723 Accuracy:  0.8706176961602671\n",
      "Epoch:  45 Loss:  5396.40722108717 Accuracy:  0.8706176961602671\n",
      "Epoch:  46 Loss:  5396.366484786167 Accuracy:  0.8706176961602671\n",
      "Epoch:  47 Loss:  5396.376100803249 Accuracy:  0.8706176961602671\n",
      "Epoch:  48 Loss:  5396.221887101747 Accuracy:  0.8708959376739009\n",
      "Epoch:  49 Loss:  5396.1748458374495 Accuracy:  0.8708959376739009\n",
      "Epoch:  50 Loss:  5396.069884435241 Accuracy:  0.8708959376739009\n",
      "Epoch:  51 Loss:  5395.963674987494 Accuracy:  0.8708959376739009\n",
      "Epoch:  52 Loss:  5395.894543612089 Accuracy:  0.8708959376739009\n",
      "Epoch:  53 Loss:  5395.782474336528 Accuracy:  0.8708959376739009\n",
      "Epoch:  54 Loss:  5395.598150259297 Accuracy:  0.8708959376739009\n",
      "Epoch:  55 Loss:  5395.5883904260745 Accuracy:  0.8708959376739009\n",
      "Epoch:  56 Loss:  5395.408059608303 Accuracy:  0.8708959376739009\n",
      "Epoch:  57 Loss:  5395.281360136014 Accuracy:  0.8708959376739009\n",
      "Epoch:  58 Loss:  5395.144434567271 Accuracy:  0.8708959376739009\n",
      "Epoch:  59 Loss:  5394.9894164234565 Accuracy:  0.8708959376739009\n",
      "Epoch:  60 Loss:  5394.790897135071 Accuracy:  0.8708959376739009\n",
      "Epoch:  61 Loss:  5394.483206124161 Accuracy:  0.8708959376739009\n",
      "Epoch:  62 Loss:  5394.404461481351 Accuracy:  0.8708959376739009\n",
      "Epoch:  63 Loss:  5394.17531342825 Accuracy:  0.8708959376739009\n",
      "Epoch:  64 Loss:  5393.937191170783 Accuracy:  0.8711741791875348\n",
      "Epoch:  65 Loss:  5393.701870231653 Accuracy:  0.8714524207011686\n",
      "Epoch:  66 Loss:  5393.408200390078 Accuracy:  0.8714524207011686\n",
      "Epoch:  67 Loss:  5393.2476507602205 Accuracy:  0.8717306622148024\n",
      "Epoch:  68 Loss:  5392.9905889300635 Accuracy:  0.8717306622148024\n",
      "Epoch:  69 Loss:  5392.842479008624 Accuracy:  0.8717306622148024\n",
      "Epoch:  70 Loss:  5392.567539592684 Accuracy:  0.8720089037284363\n",
      "Epoch:  71 Loss:  5392.435895490354 Accuracy:  0.8720089037284363\n",
      "Epoch:  72 Loss:  5392.214478699262 Accuracy:  0.8720089037284363\n",
      "Epoch:  73 Loss:  5391.956737732748 Accuracy:  0.8722871452420701\n",
      "Epoch:  74 Loss:  5391.7273532601075 Accuracy:  0.8722871452420701\n",
      "Epoch:  75 Loss:  5391.504731096717 Accuracy:  0.8722871452420701\n",
      "Epoch:  76 Loss:  5391.390097657986 Accuracy:  0.8722871452420701\n",
      "Epoch:  77 Loss:  5391.030703110453 Accuracy:  0.8722871452420701\n",
      "Epoch:  78 Loss:  5390.875677952282 Accuracy:  0.8722871452420701\n",
      "Epoch:  79 Loss:  5390.68802702291 Accuracy:  0.8722871452420701\n",
      "Epoch:  80 Loss:  5390.4272526187015 Accuracy:  0.8722871452420701\n",
      "Epoch:  81 Loss:  5390.199300602723 Accuracy:  0.8725653867557039\n",
      "Epoch:  82 Loss:  5390.000716350554 Accuracy:  0.8725653867557039\n",
      "Epoch:  83 Loss:  5389.8937135382685 Accuracy:  0.8725653867557039\n",
      "Epoch:  84 Loss:  5389.595697184537 Accuracy:  0.8725653867557039\n",
      "Epoch:  85 Loss:  5389.347323971943 Accuracy:  0.8725653867557039\n",
      "Epoch:  86 Loss:  5389.07569059862 Accuracy:  0.8728436282693378\n",
      "Epoch:  87 Loss:  5388.811771916665 Accuracy:  0.8728436282693378\n",
      "Epoch:  88 Loss:  5388.4896113536415 Accuracy:  0.8728436282693378\n",
      "Epoch:  89 Loss:  5388.108555056138 Accuracy:  0.8728436282693378\n",
      "Epoch:  90 Loss:  5387.882488473926 Accuracy:  0.8731218697829716\n",
      "Epoch:  91 Loss:  5387.345789403422 Accuracy:  0.8731218697829716\n",
      "Epoch:  92 Loss:  5386.935389717336 Accuracy:  0.8731218697829716\n",
      "Epoch:  93 Loss:  5386.3273347445975 Accuracy:  0.8731218697829716\n",
      "Epoch:  94 Loss:  5385.905678510529 Accuracy:  0.8734001112966054\n",
      "Epoch:  95 Loss:  5385.522217027893 Accuracy:  0.8734001112966054\n",
      "Epoch:  96 Loss:  5385.26652369661 Accuracy:  0.8734001112966054\n",
      "Epoch:  97 Loss:  5384.999999871251 Accuracy:  0.8734001112966054\n",
      "Epoch:  98 Loss:  5384.758924786667 Accuracy:  0.8734001112966054\n",
      "Epoch:  99 Loss:  5384.501856908354 Accuracy:  0.8734001112966054\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    o_lst = []\n",
    "    l_lst = []\n",
    "    for i, data in enumerate(train_iterator, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        o_lst.append((outputs.argmax().item()))\n",
    "        l_lst.append(labels.item())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    accuracy = accuracy_score(o_lst, l_lst)\n",
    "    print(\"Epoch: \", epoch, \"Loss: \",running_loss, \"Accuracy: \" , accuracy)\n",
    "  \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i, data in enumerate(train_iterator, 0):\n",
    "    inputs, labels = data\n",
    "    o = net(inputs)\n",
    "    lst.append(o.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4, grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inti.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAGS.tokenize('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.field.Field at 0x128227150>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "split() missing 1 required positional argument: 'split_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-9439c47d7559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/sailing_env/sailing/lib/python3.7/site-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36m_split_tokenizer\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_split_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: split() missing 1 required positional argument: 'split_size'"
     ]
    }
   ],
   "source": [
    "TAGS.tokenize(torch.t[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 7]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAGS.numericalize(['A', 'S'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
