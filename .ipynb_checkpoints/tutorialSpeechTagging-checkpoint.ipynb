{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is trying to copy this following tutorial:\n",
    "* https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108c3eed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/tancredicp/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/tancredicp/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tagged Sentences 3914\n"
     ]
    }
   ],
   "source": [
    "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset  = 'universal')\n",
    "print(\"Number of Tagged Sentences\", len(tagged_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rudolph', 'NOUN'),\n",
       " ('Agnew', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('55', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " ('and', 'CONJ'),\n",
       " ('former', 'ADJ'),\n",
       " ('chairman', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Consolidated', 'NOUN'),\n",
       " ('Gold', 'NOUN'),\n",
       " ('Fields', 'NOUN'),\n",
       " ('PLC', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('was', 'VERB'),\n",
       " ('named', 'VERB'),\n",
       " ('*-1', 'X'),\n",
       " ('a', 'DET'),\n",
       " ('nonexecutive', 'ADJ'),\n",
       " ('director', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('this', 'DET'),\n",
       " ('British', 'ADJ'),\n",
       " ('industrial', 'ADJ'),\n",
       " ('conglomerate', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentence[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_ix(word, ix):\n",
    "    return torch.tensor(ix[word], dtype = torch.long)\n",
    "\n",
    "def char_to_ix(char, ix):\n",
    "    return torch.tensor(ix[char], dtype = torch.long)\n",
    "\n",
    "def tag_to_ix(tag, ix):\n",
    "    return torch.tensor(ix[tag], dtype = torch.long)\n",
    "\n",
    "def sequence_to_idx(sequences, ix):\n",
    "    return torch.tensor([ix[s] for s in sequences], dtype = torch.long)\n",
    "\n",
    "word_to_idx = {}\n",
    "tag_to_idx = {}\n",
    "char_to_idx = {}\n",
    "for sentence in tagged_sentence:\n",
    "    for word, pos_tag in sentence:\n",
    "        if word not in word_to_idx.keys():\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "        if pos_tag not in tag_to_idx.keys():\n",
    "            tag_to_idx[pos_tag] = len(tag_to_idx)\n",
    "        for char in word:\n",
    "            if char not in char_to_idx.keys():\n",
    "                char_to_idx[char] = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 12408\n",
      "Unique tags: 12\n",
      "Unique characters: 79\n"
     ]
    }
   ],
   "source": [
    "word_vocab_size = len(word_to_idx)\n",
    "tag_vocab_size = len(tag_to_idx)\n",
    "char_vocab_size = len(char_to_idx)\n",
    "\n",
    "print(\"Unique words: {}\".format(len(word_to_idx)))\n",
    "print(\"Unique tags: {}\".format(len(tag_to_idx)))\n",
    "print(\"Unique characters: {}\".format(len(char_to_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOUN': 0,\n",
       " '.': 1,\n",
       " 'NUM': 2,\n",
       " 'ADJ': 3,\n",
       " 'VERB': 4,\n",
       " 'DET': 5,\n",
       " 'ADP': 6,\n",
       " 'CONJ': 7,\n",
       " 'X': 8,\n",
       " 'ADV': 9,\n",
       " 'PRT': 10,\n",
       " 'PRON': 11}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "tr_random = random.sample(list(range(len(tagged_sentence))), int(0.90 * len(tagged_sentence)))\n",
    "\n",
    "train = [tagged_sentence[i] for i in tr_random]\n",
    "test = [tagged_sentence[i] for i in range(len(tagged_sentence)) if i not in tr_random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3522, 392)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_DIM = 128\n",
    "CHAR_EMBEDDING_DIM = 64\n",
    "WORD_HIDDEN_DIM = 128\n",
    "CHAR_HIDDEN_DIM = 128\n",
    "EPOCHS = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualLSTMTagger(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, word_hidden_dim, char_embedding_dim, char_hidden_dim, word_vocab_size, char_vocab_size, tag_vocab_size):\n",
    "        super(DualLSTMTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, word_hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(word_hidden_dim, tag_vocab_size)\n",
    "        \n",
    "    def forward(self, sentence, words):\n",
    "        embeds = self.word_embedding(sentence)\n",
    "        char_hidden_final = []\n",
    "        for word in words:\n",
    "            char_embeds = self.char_embedding(word)\n",
    "            _, (char_hidden, char_cell_state) = self.char_lstm(char_embeds.view(len(word), 1, -1))\n",
    "            word_char_hidden_state = char_hidden.view(-1)\n",
    "            char_hidden_final.append(word_char_hidden_state)\n",
    "        char_hidden_final = torch.stack(tuple(char_hidden_final))\n",
    "        \n",
    "        combined = torch.cat((embeds, char_hidden_final), 1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(combined.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        \n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a check on the model before training.\n",
      "Sentences:\n",
      "everybody eat the food . I kept looking out the window , trying to find the one I was waiting for .\n",
      "[('everybody', '.'), ('eat', 'X'), ('the', 'ADJ'), ('food', 'CONJ'), ('.', 'ADJ'), ('I', 'VERB'), ('kept', 'ADJ'), ('looking', 'ADJ'), ('out', 'VERB'), ('the', 'VERB'), ('window', 'CONJ'), (',', '.'), ('trying', 'VERB'), ('to', 'VERB'), ('find', 'VERB'), ('the', 'ADJ'), ('one', 'CONJ'), ('I', 'VERB'), ('was', 'CONJ'), ('waiting', 'VERB'), ('for', 'DET'), ('.', 'ADJ')]\n",
      "Training Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tancredicp/Desktop/sailing_env/sailing/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/tancredicp/Desktop/sailing_env/sailing/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/tancredicp/Desktop/sailing_env/sailing/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/tancredicp/Desktop/sailing_env/sailing/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/tancredicp/Desktop/sailing_env/sailing/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/tancredicp/Desktop/sailing_env/sailing/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Running;\t11.0% Complete\r"
     ]
    }
   ],
   "source": [
    "model = DualLSTMTagger(WORD_EMBEDDING_DIM, WORD_HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, word_vocab_size, char_vocab_size, tag_vocab_size)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Define the loss function as the Negative Log Likelihood loss (NLLLoss)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# We will be using a simple SGD optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# The test sentence\n",
    "seq = \"everybody eat the food . I kept looking out the window , trying to find the one I was waiting for .\".split()\n",
    "print(\"Running a check on the model before training.\\nSentences:\\n{}\".format(\" \".join(seq)))\n",
    "with torch.no_grad():\n",
    "    words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in seq]\n",
    "    sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), dtype=torch.long).to(device)\n",
    "        \n",
    "    tag_scores = model(sentence, words)\n",
    "    _, indices = torch.max(tag_scores, 1)\n",
    "    ret = []\n",
    "    for i in range(len(indices)):\n",
    "        for key, value in tag_to_idx.items():\n",
    "            if indices[i] == value:\n",
    "                ret.append((seq[i], key))\n",
    "    print(ret)\n",
    "# Training start\n",
    "print(\"Training Started\")\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "interval = round(len(train) / 100.)\n",
    "epochs = EPOCHS\n",
    "e_interval = round(epochs / 10.)\n",
    "for epoch in range(epochs):\n",
    "    acc = 0 #to keep track of accuracy\n",
    "    loss = 0 # To keep track of the loss value\n",
    "    i = 0\n",
    "    for sentence_tag in train:\n",
    "        i += 1\n",
    "        words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in sentence_tag]\n",
    "        sentence = [s[0] for s in sentence_tag]\n",
    "        sentence = torch.tensor(sequence_to_idx(sentence, word_to_idx), dtype=torch.long).to(device)\n",
    "        targets = [s[1] for s in sentence_tag]\n",
    "        targets = torch.tensor(sequence_to_idx(targets, tag_to_idx), dtype=torch.long).to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        tag_scores = model(sentence, words)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.item()\n",
    "        _, indices = torch.max(tag_scores, 1)\n",
    "#         print(indices == targets)\n",
    "        acc += torch.mean(torch.tensor(targets == indices, dtype=torch.float))\n",
    "        if i % interval == 0:\n",
    "            print(\"Epoch {} Running;\\t{}% Complete\".format(epoch + 1, i / interval), end = \"\\r\", flush = True)\n",
    "    loss = loss / len(train)\n",
    "    acc = acc / len(train)\n",
    "    loss_list.append(float(loss))\n",
    "    accuracy_list.append(float(acc))\n",
    "    if (epoch + 1) % e_interval == 0:\n",
    "        print(\"Epoch {} Completed,\\tLoss {}\\tAccuracy: {}\".format(epoch + 1, np.mean(loss_list[-e_interval:]), np.mean(accuracy_list[-e_interval:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0500, -0.9256, -1.3714],\n",
      "        [-1.0188, -0.9739, -1.3419],\n",
      "        [-1.1330, -0.9662, -1.2126],\n",
      "        [-1.1818, -0.9763, -1.1501],\n",
      "        [-1.0766, -0.9916, -1.2439]])\n",
      "tensor([[-0.3892, -1.2426, -3.3890],\n",
      "        [-2.1082, -0.1328, -5.8464],\n",
      "        [-3.0852, -5.9469, -0.0495],\n",
      "        [-0.0499, -3.4414, -4.0961],\n",
      "        [-2.4540, -0.0929, -5.8799]])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
